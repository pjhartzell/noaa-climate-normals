{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import dask\n",
    "import dask.dataframe\n",
    "import dask_geopandas\n",
    "import geopandas\n",
    "import pandas\n",
    "\n",
    "from stactools.noaa_climate_normals.tabular import constants\n",
    "from stactools.noaa_climate_normals.tabular.parquet import load_column_metadata\n",
    "from stactools.noaa_climate_normals.tabular.utils import id_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_datatypes(\n",
    "    frequency: constants.Frequency, period: constants.Period\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Generates a dictionary of pandas datatypes for the CSV file columns from\n",
    "    metadata stored in JSON files.\"\"\"\n",
    "\n",
    "    column_metadata = load_column_metadata(frequency, period)\n",
    "    dtypes = {}\n",
    "    for key, value in column_metadata.items():\n",
    "        if value[\"pandas_dtype\"] == \"category\":\n",
    "            dtypes[key] = pandas.CategoricalDtype(value[\"categories\"], ordered=False)\n",
    "        else:\n",
    "            dtypes[key] = value[\"pandas_dtype\"]\n",
    "    return dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet(\n",
    "    csv_hrefs: List[str],\n",
    "    frequency: constants.Frequency,\n",
    "    period: constants.Period,\n",
    "    parquet_path: str,\n",
    ") -> str:\n",
    "    \"\"\"Creates a GeoParquet file from a list of CSV files.\"\"\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def dataframe_from_csv(\n",
    "        csv_href: str, pd_dtypes: Dict[str, Any], empty_df: pandas.DataFrame\n",
    "    ) -> pandas.DataFrame:\n",
    "        \"\"\"Returns a dataframe with an ordered, complete set of columns.\"\"\"\n",
    "        df = pandas.read_csv(csv_href, dtype=pd_dtypes)\n",
    "        return pandas.concat([df, empty_df])[empty_df.columns]\n",
    "\n",
    "    pd_dtypes = pandas_datatypes(frequency, period)\n",
    "    empty_df = pandas.DataFrame({c: pandas.Series(dtype=t) for c, t in pd_dtypes.items()})\n",
    "\n",
    "    pandas_dataframes = [\n",
    "        dataframe_from_csv(csv_href, pd_dtypes, empty_df) for csv_href in csv_hrefs\n",
    "    ]\n",
    "\n",
    "    dask_dataframe = dask.dataframe.from_delayed(pandas_dataframes, meta=empty_df)\n",
    "\n",
    "    dask_geodataframe = dask_geopandas.from_dask_dataframe(dask_dataframe)\n",
    "\n",
    "    # This produces \"ValueError: Length of values (x) does not match length of\n",
    "    # index (y)\". Appears the delayed dataframe only has a few rows, but\n",
    "    # `points_from_xy` has generated data for all rows, which also seems to\n",
    "    # break the delayed concept. If I comment this out, parquet files are generated.\n",
    "    dask_geodataframe.assign(\n",
    "        geometry=lambda df: geopandas.points_from_xy(\n",
    "            x=df.LONGITUDE,\n",
    "            y=df.LATITUDE,\n",
    "            z=df.ELEVATION,\n",
    "            crs=constants.CRS,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dask_geodataframe.repartition(10).to_parquet(parquet_path, write_index=False)\n",
    "\n",
    "    return parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (100) does not match length of index (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m period \u001b[39m=\u001b[39m constants\u001b[39m.\u001b[39mPeriod(csv_dir\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\n\u001b[1;32m      9\u001b[0m frequency \u001b[39m=\u001b[39m constants\u001b[39m.\u001b[39mFrequency(csv_dir\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m create_parquet(\n\u001b[1;32m     12\u001b[0m     csv_hrefs\u001b[39m=\u001b[39;49mcsv_hrefs[\u001b[39m0\u001b[39;49m:\u001b[39m100\u001b[39;49m],\n\u001b[1;32m     13\u001b[0m     frequency\u001b[39m=\u001b[39;49mfrequency,\n\u001b[1;32m     14\u001b[0m     period\u001b[39m=\u001b[39;49mperiod,\n\u001b[1;32m     15\u001b[0m     parquet_path\u001b[39m=\u001b[39;49mPath(\u001b[39m\"\u001b[39;49m\u001b[39mgeoparquet_test\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mid_string(frequency, period)\u001b[39m}\u001b[39;49;00m\u001b[39m.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     16\u001b[0m )\n",
      "Cell \u001b[0;32mIn [20], line 28\u001b[0m, in \u001b[0;36mcreate_parquet\u001b[0;34m(csv_hrefs, frequency, period, parquet_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m dask_dataframe \u001b[39m=\u001b[39m dask\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39mfrom_delayed(pandas_dataframes, meta\u001b[39m=\u001b[39mempty_df)\n\u001b[1;32m     26\u001b[0m dask_geodataframe \u001b[39m=\u001b[39m dask_geopandas\u001b[39m.\u001b[39mfrom_dask_dataframe(dask_dataframe)\n\u001b[0;32m---> 28\u001b[0m dask_geodataframe\u001b[39m.\u001b[39;49massign(\n\u001b[1;32m     29\u001b[0m     geometry\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m df: geopandas\u001b[39m.\u001b[39;49mpoints_from_xy(\n\u001b[1;32m     30\u001b[0m         x\u001b[39m=\u001b[39;49mdf\u001b[39m.\u001b[39;49mLONGITUDE,\n\u001b[1;32m     31\u001b[0m         y\u001b[39m=\u001b[39;49mdf\u001b[39m.\u001b[39;49mLATITUDE,\n\u001b[1;32m     32\u001b[0m         z\u001b[39m=\u001b[39;49mdf\u001b[39m.\u001b[39;49mELEVATION,\n\u001b[1;32m     33\u001b[0m         crs\u001b[39m=\u001b[39;49mconstants\u001b[39m.\u001b[39;49mCRS,\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m dask_geodataframe\u001b[39m.\u001b[39mrepartition(\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto_parquet(parquet_path, write_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m parquet_path\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/dask/dataframe/core.py:5086\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   5083\u001b[0m     pairs \u001b[39m=\u001b[39m [k, kwargs[k]]\n\u001b[1;32m   5085\u001b[0m     \u001b[39m# Figure out columns of the output\u001b[39;00m\n\u001b[0;32m-> 5086\u001b[0m     df2 \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49m_meta_nonempty\u001b[39m.\u001b[39;49massign(\n\u001b[1;32m   5087\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_extract_meta({k: kwargs[k]}, nonempty\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   5088\u001b[0m     )\n\u001b[1;32m   5089\u001b[0m     data \u001b[39m=\u001b[39m elemwise(methods\u001b[39m.\u001b[39massign, data, \u001b[39m*\u001b[39mpairs, meta\u001b[39m=\u001b[39mdf2)\n\u001b[1;32m   5091\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/pandas/core/frame.py:4878\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   4875\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   4877\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m-> 4878\u001b[0m     data[k] \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(v, data)\n\u001b[1;32m   4879\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/geopandas/geodataframe.py:1443\u001b[0m, in \u001b[0;36mGeoDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mGeometry column does not contain geometry.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1443\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__setitem__\u001b[39;49m(key, value)\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/pandas/core/frame.py:3977\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3976\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3977\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/pandas/core/frame.py:4171\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4162\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4163\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4169\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4170\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4171\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4173\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4174\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4175\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4177\u001b[0m     ):\n\u001b[1;32m   4178\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4179\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/pandas/core/frame.py:4904\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4901\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4903\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4904\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4905\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/noaa-climate-normals/.venv/lib/python3.9/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (100) does not match length of index (2)"
     ]
    }
   ],
   "source": [
    "# NOTE: Only the \"annualseasonal/2006-2020\" data has updated JSON metadata. You\n",
    "# can't run any of the other CSV data piles yet.\n",
    "\n",
    "csv_dir = \"/Volumes/Samsung_T5/data/ncn/tabular/normals-annualseasonal/2006-2020/access\"\n",
    "csv_paths = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "csv_hrefs = [path.as_posix() for path in csv_paths]\n",
    "\n",
    "period = constants.Period(csv_dir.split(\"/\")[-2])\n",
    "frequency = constants.Frequency(csv_dir.split(\"/\")[-3].split(\"-\")[1])\n",
    "\n",
    "create_parquet(\n",
    "    csv_hrefs=csv_hrefs[0:100],\n",
    "    frequency=frequency,\n",
    "    period=period,\n",
    "    parquet_path=Path(\"geoparquet_test\", f\"{id_string(frequency, period)}.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.read_parquet(\"geoparquet_test/2006_2020-annualseasonal.parquet\")\n",
    "gdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79c59cb5a6acf95f9879c4c295cb5bb11e318f4385f6b6f93afa3e88c52c85ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
